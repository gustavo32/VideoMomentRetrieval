{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Próximos passos</h3>\n",
    "<ul>\n",
    "    <div style=\"list-style: none; padding-inline-start: 0px !important; margin-block-start: 0em !important;\">\n",
    "        <strike>✓ Pegar os conjuntos de frames e aplicar a distância de cosseno com a sentença para a geração da matriz de co-atenção</strike><br/>\n",
    "        <strike>✓ Gerar as representações de co-atenção</strike><br/>\n",
    "        <strike>✓ Gerar a similaridade entre o segmento de vídeo e sentença com atenção</strike><br/>\n",
    "        <strike>✓ Construir uma função que processe os dados on the fly</strike><br/>\n",
    "        <strike>✓ Construir um batch com mais de um exemplo (necessário)</strike><br/>\n",
    "        <strike>✓ Gerar os top-K hard exemplos negativos</strike><br/>\n",
    "        <strike>✓ Construir a função margin-based ranking loss</strike><br/>\n",
    "        <strike>✓ Elaborar a média móvel dos pares de video e sentença com atenção</strike><br/>\n",
    "        <strike>✓ Dividir pipelines de treino e teste</strike><br/>\n",
    "        <strike>✓ Construir a métrica mIOU</strike><br/>\n",
    "        <strike>✓ Investigar os NANs apresentados no video, sentença (problema começa na segunda cnn, mas sem motivo aparente)</strike><br/>\n",
    "        <strike>✓ Construir o train_step e o test_step</strike><br/>\n",
    "        <strike>✓ Construir preprocessamento para coletar a anotação mais fidedigna</strike><br/>\n",
    "        <strike>✓ Passar o conjunto de dados de forma automática ao pipeline</strike><br/>\n",
    "        <strike>✓ Passar as funções do modelo de numpy para tensorflow</strike><br/>\n",
    "        <strike>✓ Passar as funções custo de numpy para tensorflow</strike><br/>\n",
    "    </div>\n",
    "    <li>Organizar o código em módulos</li>\n",
    "    <li>Construir a métrica Recall@N</li>\n",
    "    <li>Rodar uma primeira versão do treinamento</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python: can't open file 'utils/teste.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!python utils/teste.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv3D, MaxPool3D, Flatten, Dense, Layer, Bidirectional\n",
    "from tensorflow.keras.layers import Dropout, Input, BatchNormalization, Embedding, GRU\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow import convert_to_tensor, split, expand_dims\n",
    "import tensorflow as tf\n",
    "from utils.losses import margin_based_ranking_loss\n",
    "from utils.metrics import mIOU, prediction_by_moving_average\n",
    "# from utils.frames import load_transform_video, mapped_load_transform_video\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "import re\n",
    "import tempfile\n",
    "import ssl\n",
    "import cv2\n",
    "from utils import embed\n",
    "\n",
    "import imageio\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/train_data.json', 'r') as f:\n",
    "    train_data = pd.Series(json.load(f)).apply(pd.Series)[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_gif(images):\n",
    "    converted_images = np.clip(images * 255, 0, 255).astype(np.uint8)\n",
    "    imageio.mimsave('./animation.gif', converted_images, fps=30)\n",
    "    return embed.embed_file('./animation.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_video(path, max_frames=900, min_frames=900, resize=(112, 112)):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = np.empty(resize+tuple([3]))[None]\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "#             frame = tf.numpy_function(crop_center_square, [frame], [tf.float32])\n",
    "\n",
    "            y, x = frame.shape[0:2]\n",
    "            min_dim = min(y, x)\n",
    "            start_x = (x // 2) - (min_dim // 2)\n",
    "            start_y = (y // 2) - (min_dim // 2)\n",
    "            frame = frame[start_y:start_y+min_dim,start_x:start_x+min_dim]\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = frame[:, :, [2, 1, 0]][None]\n",
    "            frames = np.vstack([frames, frame])\n",
    "\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "            \n",
    "        while len(frames) < min_frames or (len(frames) > min_frames and len(frames) < max_frames):\n",
    "            frames = np.vstack([frames, np.zeros(resize + tuple([3]))[None]])\n",
    "        \n",
    "    except:\n",
    "        print(frame)\n",
    "        \n",
    "    finally:\n",
    "        cap.release()\n",
    "    \n",
    "    frames = (frames / 255.0).astype(np.float32)\n",
    "    frames = np.where((frames == -np.inf) | (frames == np.inf) | (pd.isna(frames)) | (frames < 0), 0.0, frames)\n",
    "#     frames[(frames == -np.inf) | (frames == np.inf) | (pd.isna(frames))] = 0.0\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_matrix(descriptions, embedding_dim, n_tokens):\n",
    "    \n",
    "    vectorizer = TextVectorization(max_tokens=n_tokens, output_sequence_length=25)\n",
    "    text_ds = Dataset.from_tensor_slices(descriptions).batch(16)\n",
    "    vectorizer.adapt(text_ds)\n",
    "\n",
    "    path_to_glove_file = \"utils/vocabulary/glove.6B.{}d.txt\".format(embedding_dim)\n",
    "\n",
    "    embeddings_index = {}\n",
    "    with open(path_to_glove_file, 'rb') as f:\n",
    "        data = [line.split(maxsplit=1) for line in f]\n",
    "    pretrained_embeddings = pd.DataFrame.from_records(data, columns=['word', 'coefs'])\n",
    "    pretrained_embeddings['word'] = pretrained_embeddings['word'].apply(lambda x: x.decode())\n",
    "    pretrained_embeddings['coefs'] = pretrained_embeddings['coefs'].apply(lambda x: np.fromstring(x, \"f\", sep=\" \"))\n",
    "    pretrained_embeddings = pretrained_embeddings.set_index('word', drop=True).iloc[:, 0]\n",
    "\n",
    "    voc = vectorizer.get_vocabulary()\n",
    "    num_tokens = len(voc) + 2\n",
    "    word_index = dict(zip(voc, range(len(voc))))\n",
    "    hits = 0\n",
    "    misses = 0\n",
    "\n",
    "    embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = pretrained_embeddings[word]\n",
    "            hits += 1\n",
    "        except KeyError:\n",
    "            misses += 1\n",
    "            pass\n",
    "    return vectorizer, embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['video'] = 'datasets/DiDeMo/' + train_data['video'] + '.mp4'\n",
    "paths = tf.data.Dataset.from_tensor_slices(train_data['video'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(tf.data.Dataset):\n",
    "    def _generator(paths, n_frames, size):\n",
    "        for path in paths:\n",
    "            yield load_video(path.decode(), n_frames, n_frames, resize=tuple(size))\n",
    "\n",
    "    def __new__(cls, paths, n_frames, size=(112,112)):\n",
    "        return tf.data.Dataset.from_generator(\n",
    "            cls._generator,\n",
    "            output_types=tf.dtypes.float32,\n",
    "            output_shapes=[n_frames] + list(size) + [3],\n",
    "            args=(paths, n_frames, size)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = VideoDataset(train_data['video'].values, 900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoLayer(Layer):\n",
    "    def __init__(self):\n",
    "        super(VideoLayer, self).__init__()\n",
    "        self.conv3d_1 = Conv3D(32, kernel_size=7, activation='tanh')\n",
    "        self.conv3d_2 = Conv3D(32, kernel_size=5, activation='tanh')\n",
    "        self.conv3d_3 = Conv3D(32, kernel_size=3, activation='tanh')\n",
    "        \n",
    "        self.maxpool3d_1 = MaxPool3D(pool_size=3)\n",
    "        self.maxpool3d_2 = MaxPool3D(pool_size=3)\n",
    "        self.maxpool3d_3 = MaxPool3D(pool_size=3)\n",
    "        \n",
    "#         self.bn_1 = BatchNormalization()\n",
    "        self.flatten_1 = Flatten()\n",
    "        \n",
    "        self.dense_1 = Dense(units=128, activation='tanh')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.conv3d_1(inputs)\n",
    "        x = self.maxpool3d_1(x)\n",
    "        x = self.conv3d_2(x)\n",
    "        x = self.maxpool3d_2(x)\n",
    "        x = self.conv3d_3(x)\n",
    "        x = self.maxpool3d_3(x)\n",
    "#         x = self.bn_1(x)\n",
    "        x = self.flatten_1(x)\n",
    "        return self.dense_1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceLayer(Layer):\n",
    "    def __init__(self, n_tokens, embedding_dim, embedding_matrix):\n",
    "        super(SentenceLayer, self).__init__()\n",
    "        self.embedding_1 = Embedding(\n",
    "            n_tokens,\n",
    "            embedding_dim,\n",
    "            embeddings_initializer=initializers.Constant(embedding_matrix),\n",
    "            trainable=False\n",
    "        )\n",
    "        \n",
    "        self.bigru_1 = Bidirectional(GRU(64, return_sequences=True))\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.embedding_1(inputs)\n",
    "        return self.bigru_1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MomentVideo(Model):\n",
    "    def __init__(self, frame_rate, n_tokens, embedding_dim, embedding_matrix=None, batch_size=8):\n",
    "        super(MomentVideo, self).__init__()\n",
    "        self.video_1 = VideoLayer()\n",
    "        self.sentence_1 = SentenceLayer(n_tokens, embedding_dim, embedding_matrix)\n",
    "        self.frame_rate = frame_rate\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    \n",
    "    def cosine_similarity(self, tensor1, tensor2):\n",
    "        num = tf.linalg.matmul(expand_dims(tensor1, 1), expand_dims(tensor2, 1), transpose_a=True)\n",
    "        den = tf.norm(tensor1)*tf.norm(tensor2)\n",
    "        return num/(den+1e-15)\n",
    "    \n",
    "    \n",
    "    def matrix_cosine_similarity(self, tensor1, tensor2):\n",
    "        matrix = []\n",
    "        for i in range(tensor1.shape[0]):\n",
    "            row = []\n",
    "            for j in range(tensor2.shape[0]):\n",
    "                row.append(self.cosine_similarity(tensor1[i, :], tensor2[j, :]))\n",
    "            matrix.append(row)\n",
    "        return tf.stack(matrix)[:,:,0,0]\n",
    "    \n",
    "    \n",
    "    def similarity_between_repr_and_attend(self, tensor1, tensor2):\n",
    "        scores = []\n",
    "        for k in range(tensor1.shape[0]):\n",
    "            scores.append(self.cosine_similarity(tensor1[k, :], tensor2[k, :]))\n",
    "        return tf.stack(scores)[:, 0, 0]\n",
    "    \n",
    "    \n",
    "    def get_scores(self, video_repr_tensor, sentence_repr_tensor):\n",
    "        coattention_matrix = self.matrix_cosine_similarity(video_repr_tensor, sentence_repr_tensor)\n",
    "        \n",
    "        normalized_sentence = tf.nn.softmax(coattention_matrix, axis=1)\n",
    "        normalized_video = tf.nn.softmax(coattention_matrix, axis=0)\n",
    "            \n",
    "        matrix = []\n",
    "        for i in range(video_repr_tensor.shape[0]):\n",
    "            row_sum = np.zeros((128))\n",
    "            for j in range(sentence_repr_tensor.shape[0]):\n",
    "                row_sum += normalized_sentence[i][j] * sentence_repr_tensor[j, :]\n",
    "            matrix.append(row_sum)\n",
    "            \n",
    "        sentence_attention = tf.stack(matrix)\n",
    "    \n",
    "        matrix = []\n",
    "        for j in range(sentence_repr_tensor.shape[0]):\n",
    "            row_sum = np.zeros((128))\n",
    "            for i in range(video_repr_tensor.shape[0]):\n",
    "                row_sum += normalized_video[i][j] * video_repr_tensor[i, :]\n",
    "            matrix.append(row_sum)\n",
    "            \n",
    "        video_attention = tf.stack(matrix)\n",
    "        \n",
    "        scores_video = self.similarity_between_repr_and_attend(video_repr_tensor, sentence_attention)\n",
    "        scores_sentence = self.similarity_between_repr_and_attend(sentence_repr_tensor, video_attention)\n",
    "        \n",
    "        return scores_video, scores_sentence\n",
    "    \n",
    "    \n",
    "    def call(self, data, training=False):\n",
    "        \n",
    "        videos, sentences = data\n",
    "        #extract the features\n",
    "        video_repr = []\n",
    "        for last_frame in range(videos.shape[1] // self.frame_rate):\n",
    "            video_repr.append(self.video_1(videos[:, last_frame*self.frame_rate:(last_frame+1)*self.frame_rate]))\n",
    "        videos_repr = tf.stack(video_repr, axis=1)\n",
    "        sentences_repr = self.sentence_1(sentences)\n",
    "#         print(videos_repr[(videos_repr == -np.inf) | (videos_repr == np.inf) | (pd.isna(videos_repr))])\n",
    "        \n",
    "        if training == False:\n",
    "            scores_videos = []\n",
    "            scores_sentences = []\n",
    "            for i in range(self.batch_size):\n",
    "                scores_video, scores_sentence = self.get_scores(videos_repr[i], sentences_repr[i])\n",
    "                scores_videos.append(scores_video)\n",
    "                scores_sentences.append(scores_sentence)\n",
    "            \n",
    "            scores_videos = np.vstack(scores_videos)\n",
    "            scores_sentences = np.vstack(scores_sentences)\n",
    "            \n",
    "            return scores_videos, scores_sentences\n",
    "        \n",
    "        else:\n",
    "            sum_scores_videos = []\n",
    "            sum_scores_sentences = []\n",
    "            for i in range(self.batch_size):\n",
    "                _sum_scores_videos = []\n",
    "                _sum_scores_sentences = []\n",
    "                for j in range(self.batch_size):\n",
    "                    scores_video, scores_sentence = self.get_scores(videos_repr[i], sentences_repr[j])\n",
    "                    _sum_scores_videos.append(tf.reduce_sum(scores_video)/len(scores_video))\n",
    "                    _sum_scores_sentences.append(tf.reduce_sum(scores_sentence)/len(scores_sentence))\n",
    "                \n",
    "                sum_scores_videos.append(_sum_scores_videos)\n",
    "                sum_scores_sentences.append(_sum_scores_sentences)\n",
    "            \n",
    "            sum_scores_videos = tf.stack(sum_scores_videos)\n",
    "            sum_scores_sentences = tf.transpose(tf.stack(sum_scores_sentences))\n",
    "            \n",
    "            return sum_scores_videos, sum_scores_sentences\n",
    "    \n",
    "    \n",
    "    def train_step(self, data):\n",
    "            videos, sentences, y_true = data\n",
    "#             videos, sentences, y_true = [(v, s, l) for (v, s, l) in data.take(1)]\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                scores_videos, scores_sentences = self((videos, sentences), training=True)  # Forward pass\n",
    "                # Compute the loss margin-based ranking loss\n",
    "                loss = margin_based_ranking_loss(scores_videos, scores_sentences, margin=1, top_k=2)\n",
    "\n",
    "            # Compute gradients\n",
    "            trainable_vars = self.trainable_variables\n",
    "            gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "            # Update weights\n",
    "            self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "#             # Compute our own metrics\n",
    "#             loss_tracker.update_state(loss)\n",
    "#             mae_metric.update_state(y, y_pred)\n",
    "            return {\"loss\": loss}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        (videos, sentences), y_true = data\n",
    "        scores_videos, scores_sentences = self((videos, sentences), training=False)\n",
    "        \n",
    "        return {'mIOU': mIOU(scores_videos, y_true)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_agreed_labels(labels):\n",
    "    str_labels = [str(start) + ':' + str(end) for start, end in labels]\n",
    "    uniques, idx, counts = np.unique(str_labels, return_counts=True, return_index=True)\n",
    "    idx_max = np.argmax(counts)\n",
    "    agreed_labels = labels[idx[idx_max]]\n",
    "    return pd.Series(agreed_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "vectorizer, embedding_matrix = get_embedding_matrix(train_data['description'], embedding_dim, 20000)\n",
    "descriptions = tf.data.Dataset.from_tensor_slices(vectorizer(train_data['description']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train_data['times']\n",
    "labels = labels.apply(most_agreed_labels)\n",
    "labels_ds = tf.data.Dataset.from_tensor_slices(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "dataset = tf.data.Dataset.zip((videos, descriptions, labels_ds)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iter_videos = iter(videos.batch(4))\n",
    "# batch_video = iter_videos.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\luis_\\documents\\tcc\\tcc_env\\lib\\site-packages\\ipykernel_launcher.py:33: RuntimeWarning: invalid value encountered in less\n"
     ]
    }
   ],
   "source": [
    "videos, sentences, labels = iter(dataset).get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('gpu:0'):\n",
    "    moment = MomentVideo(75, len(embedding_matrix), embedding_dim, embedding_matrix, batch_size=batch_size)\n",
    "    scores_videos, scores_sentences = moment((videos, sentences), training=True)\n",
    "#     moment.compile()\n",
    "#     moment.fit(dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.1270995, 4.092556 , 4.042369 , 3.94649  ], dtype=float32)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy_margin_based_ranking_loss(scores_videos.numpy(), scores_sentences.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([4.1270995, 4.092556 , 4.042369 , 3.94649  ], dtype=float32)>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_margin_based_ranking_loss(scores_videos, scores_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranking_loss(scores, margin, top_k=2):\n",
    "    shape_negative = list(scores.shape)\n",
    "    shape_negative[-1] = shape_negative[-1] - 1\n",
    "    \n",
    "    scores_true = tf.linalg.diag_part(scores)\n",
    "    scores_negatives = tf.sort(tf.reshape(scores[scores != tf.transpose(scores)], shape_negative), axis=1)[:, ::-1][:, :top_k]\n",
    "               \n",
    "    loss = margin - tf.tile(tf.reshape(scores_true, (-1, 1)), [1, scores_negatives.shape[1]]) + scores_negatives\n",
    "    loss = tf.where(loss < 0, 0, loss)\n",
    "    loss = tf.reduce_sum(loss, axis=1)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def tf_margin_based_ranking_loss(scores_videos, scores_sentence, margin=1, top_k=2):\n",
    "    \n",
    "    video_loss = get_ranking_loss(scores_videos, margin, top_k)\n",
    "    sentence_loss = get_ranking_loss(scores_sentences, margin, top_k)\n",
    "    \n",
    "    return video_loss + sentence_loss\n",
    "    \n",
    "def numpy_margin_based_ranking_loss(scores_video, scores_sentence, margin=1, top_k=2):\n",
    "    \n",
    "    shape_negative = list(scores_video.shape)\n",
    "    shape_negative[-1] = shape_negative[-1] - 1\n",
    "\n",
    "    score_true_videos = scores_video.diagonal()\n",
    "    scores_negatives_videos = np.sort(scores_video[scores_video != scores_video.T].reshape(shape_negative), axis=1)[:, ::-1][:, :top_k]\n",
    "    \n",
    "    video_loss = 1 - np.tile(score_true_videos.reshape(-1, 1), scores_negatives_videos.shape[1]) + scores_negatives_videos\n",
    "    video_loss = np.where(video_loss < 0, 0, video_loss)\n",
    "    video_loss = np.sum(video_loss, axis=1)\n",
    "\n",
    "    shape_negative = list(scores_sentence.shape)\n",
    "    shape_negative[-1] = shape_negative[-1] - 1\n",
    "\n",
    "    score_true_sentences = scores_sentence.diagonal()\n",
    "    scores_negatives_sentences = np.sort(scores_sentence[scores_sentence != scores_sentence.T].reshape(shape_negative), axis=1)[:, ::-1][:, :top_k]\n",
    "\n",
    "    sentence_loss = 1 - np.tile(score_true_sentences.reshape(-1, 1), scores_negatives_sentences.shape[1]) + scores_negatives_sentences\n",
    "    sentence_loss = np.where(sentence_loss < 0, 0, sentence_loss)\n",
    "    sentence_loss = np.sum(sentence_loss, axis=1)\n",
    "\n",
    "    return video_loss + sentence_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Exploratory Analysis over Video Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_segments</th>\n",
       "      <th>description</th>\n",
       "      <th>dl_link</th>\n",
       "      <th>times</th>\n",
       "      <th>video</th>\n",
       "      <th>annotation_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>a brown rat goes into someone's hand then onto...</td>\n",
       "      <td>https://www.flickr.com/video_download.gne?id=2...</td>\n",
       "      <td>[[2, 2], [2, 2], [2, 2], [2, 2]]</td>\n",
       "      <td>54322086@N00_2408598493_274c77d26a.avi</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>an orange kitten is sitting then gets up and w...</td>\n",
       "      <td>https://www.flickr.com/video_download.gne?id=2...</td>\n",
       "      <td>[[5, 5], [5, 5], [5, 5], [5, 5]]</td>\n",
       "      <td>99051133@N00_2502628368_d14bd317de.mov</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>a person walks outside and then back in</td>\n",
       "      <td>https://www.flickr.com/video_download.gne?id=5...</td>\n",
       "      <td>[[3, 3], [3, 3], [3, 3], [3, 4]]</td>\n",
       "      <td>67801451@N00_5358663022_243bd90fbc.mov</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>the guards spin around 180 degrees</td>\n",
       "      <td>https://www.flickr.com/video_download.gne?id=4...</td>\n",
       "      <td>[[4, 4], [4, 5], [3, 4], [3, 4]]</td>\n",
       "      <td>64379474@N00_4479342537_7b5a3d3f1d.avi</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>the plane flies off the roof</td>\n",
       "      <td>https://www.flickr.com/video_download.gne?id=9...</td>\n",
       "      <td>[[5, 5], [4, 5], [4, 5], [4, 5]]</td>\n",
       "      <td>63122283@N06_9978694646_e72011157f.mov</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33000</th>\n",
       "      <td>6</td>\n",
       "      <td>man holds his hand up to feed the birds for th...</td>\n",
       "      <td>https://www.flickr.com/video_download.gne?id=3...</td>\n",
       "      <td>[[1, 1], [1, 1], [1, 1], [1, 1]]</td>\n",
       "      <td>65977087@N00_3751407080_6bda50e6df.mov</td>\n",
       "      <td>64632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33001</th>\n",
       "      <td>6</td>\n",
       "      <td>someone walks across bottom screen</td>\n",
       "      <td>https://www.flickr.com/video_download.gne?id=7...</td>\n",
       "      <td>[[0, 1], [0, 0], [2, 2], [0, 0]]</td>\n",
       "      <td>89333651@N00_7564648598_776b587a55.mp4</td>\n",
       "      <td>64637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33002</th>\n",
       "      <td>6</td>\n",
       "      <td>first time we can see the musicians</td>\n",
       "      <td>https://www.flickr.com/video_download.gne?id=4...</td>\n",
       "      <td>[[2, 2], [2, 2], [2, 2], [2, 2]]</td>\n",
       "      <td>77118917@N00_4606529468_6ba24fe0c9.avi</td>\n",
       "      <td>64638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33003</th>\n",
       "      <td>6</td>\n",
       "      <td>the pitcher's mound comes into view</td>\n",
       "      <td>https://www.flickr.com/video_download.gne?id=4...</td>\n",
       "      <td>[[3, 3], [3, 3], [3, 3], [3, 3]]</td>\n",
       "      <td>72426516@N00_4868053620_46b99ec39f.mov</td>\n",
       "      <td>64641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33004</th>\n",
       "      <td>6</td>\n",
       "      <td>the dancing woman spins for the last time</td>\n",
       "      <td>https://www.flickr.com/video_download.gne?id=8...</td>\n",
       "      <td>[[2, 2], [2, 2], [0, 0], [2, 2]]</td>\n",
       "      <td>72511036@N00_8457819704_37bed14ae1.</td>\n",
       "      <td>64644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33005 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       num_segments                                        description  \\\n",
       "0                 6  a brown rat goes into someone's hand then onto...   \n",
       "1                 6  an orange kitten is sitting then gets up and w...   \n",
       "2                 6            a person walks outside and then back in   \n",
       "3                 6                 the guards spin around 180 degrees   \n",
       "4                 6                       the plane flies off the roof   \n",
       "...             ...                                                ...   \n",
       "33000             6  man holds his hand up to feed the birds for th...   \n",
       "33001             6                 someone walks across bottom screen   \n",
       "33002             6                first time we can see the musicians   \n",
       "33003             6                the pitcher's mound comes into view   \n",
       "33004             6          the dancing woman spins for the last time   \n",
       "\n",
       "                                                 dl_link  \\\n",
       "0      https://www.flickr.com/video_download.gne?id=2...   \n",
       "1      https://www.flickr.com/video_download.gne?id=2...   \n",
       "2      https://www.flickr.com/video_download.gne?id=5...   \n",
       "3      https://www.flickr.com/video_download.gne?id=4...   \n",
       "4      https://www.flickr.com/video_download.gne?id=9...   \n",
       "...                                                  ...   \n",
       "33000  https://www.flickr.com/video_download.gne?id=3...   \n",
       "33001  https://www.flickr.com/video_download.gne?id=7...   \n",
       "33002  https://www.flickr.com/video_download.gne?id=4...   \n",
       "33003  https://www.flickr.com/video_download.gne?id=4...   \n",
       "33004  https://www.flickr.com/video_download.gne?id=8...   \n",
       "\n",
       "                                  times  \\\n",
       "0      [[2, 2], [2, 2], [2, 2], [2, 2]]   \n",
       "1      [[5, 5], [5, 5], [5, 5], [5, 5]]   \n",
       "2      [[3, 3], [3, 3], [3, 3], [3, 4]]   \n",
       "3      [[4, 4], [4, 5], [3, 4], [3, 4]]   \n",
       "4      [[5, 5], [4, 5], [4, 5], [4, 5]]   \n",
       "...                                 ...   \n",
       "33000  [[1, 1], [1, 1], [1, 1], [1, 1]]   \n",
       "33001  [[0, 1], [0, 0], [2, 2], [0, 0]]   \n",
       "33002  [[2, 2], [2, 2], [2, 2], [2, 2]]   \n",
       "33003  [[3, 3], [3, 3], [3, 3], [3, 3]]   \n",
       "33004  [[2, 2], [2, 2], [0, 0], [2, 2]]   \n",
       "\n",
       "                                        video  annotation_id  \n",
       "0      54322086@N00_2408598493_274c77d26a.avi              2  \n",
       "1      99051133@N00_2502628368_d14bd317de.mov              3  \n",
       "2      67801451@N00_5358663022_243bd90fbc.mov              4  \n",
       "3      64379474@N00_4479342537_7b5a3d3f1d.avi              5  \n",
       "4      63122283@N06_9978694646_e72011157f.mov              7  \n",
       "...                                       ...            ...  \n",
       "33000  65977087@N00_3751407080_6bda50e6df.mov          64632  \n",
       "33001  89333651@N00_7564648598_776b587a55.mp4          64637  \n",
       "33002  77118917@N00_4606529468_6ba24fe0c9.avi          64638  \n",
       "33003  72426516@N00_4868053620_46b99ec39f.mov          64641  \n",
       "33004     72511036@N00_8457819704_37bed14ae1.          64644  \n",
       "\n",
       "[33005 rows x 6 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.Series(train_data).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127.99600000000646"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 150)\n",
    "df['description'].str.len().quantile(0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso significa que em 99.9% dos casos, as descrições não passaram de 128 caractereces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.0"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['description'].str.split().str.len().quantile(0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso significa que em 99.9% dos casos, as descrições não passaram de 25 palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = df['description'].str.split()\n",
    "word_counts = pd.value_counts(words.apply(pd.Series).stack())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the          24341\n",
       "a            10974\n",
       "in            8262\n",
       "to            5668\n",
       "man           4637\n",
       "             ...  \n",
       "Green,           1\n",
       "snatched         1\n",
       "filmer.          1\n",
       "harnesses        1\n",
       "master.          1\n",
       "Length: 9260, dtype: int64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claramente, as palavras mais citadas são os artigos como: 'the', 'a', 'in', 'to'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=5.0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.norm([3.0, 4.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
