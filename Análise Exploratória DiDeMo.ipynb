{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Próximos passos</h3>\n",
    "<ul>\n",
    "    <div style=\"list-style: none; padding-inline-start: 0px !important; margin-block-start: 0em !important;\">\n",
    "        <strike>✓ Pegar os conjuntos de frames e aplicar a distância de cosseno com a sentença para a geração da matriz de co-atenção</strike><br/>\n",
    "        <strike>✓ Gerar as representações de co-atenção</strike><br/>\n",
    "        <strike>✓ Gerar a similaridade entre o segmento de vídeo e sentença com atenção</strike><br/>\n",
    "        <strike>✓ Construir uma função que processe os dados on the fly</strike><br/>\n",
    "        <strike>✓ Construir um batch com mais de um exemplo (necessário)</strike><br/>\n",
    "        <strike>✓ Gerar os top-K hard exemplos negativos</strike><br/>\n",
    "        <strike>✓ Construir a função margin-based ranking loss</strike><br/>\n",
    "    </div>\n",
    "    <li>Elaborar a média móvel dos pares de video-sentença</li>\n",
    "    <li>Construir as funções de métricas</li>\n",
    "    <li>Passar o conjunto de dados de forma automática ao pipeline</li>\n",
    "    <li>Rodar uma primeira versão do treinamento</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python: can't open file 'utils/teste.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!python utils/teste.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv3D, MaxPool3D, Flatten, Dense, Layer, Bidirectional\n",
    "from tensorflow.keras.layers import Dropout, Input, BatchNormalization, Embedding, GRU\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow import convert_to_tensor, split, expand_dims\n",
    "import tensorflow as tf\n",
    "# from utils.frames import load_transform_video, mapped_load_transform_video\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "import random\n",
    "import re\n",
    "import tempfile\n",
    "import ssl\n",
    "import cv2\n",
    "from utils import embed\n",
    "\n",
    "import imageio\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/train_data.json', 'r') as f:\n",
    "    train_data = pd.Series(json.load(f)).apply(pd.Series)[:200]\n",
    "    train_data = dd.from_pandas(train_data, npartitions=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_gif(images):\n",
    "    converted_images = np.clip(images * 255, 0, 255).astype(np.uint8)\n",
    "    imageio.mimsave('./animation.gif', converted_images, fps=30)\n",
    "    return embed.embed_file('./animation.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_video(path, max_frames=900, min_frames=900, resize=(112, 112)):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = np.empty(resize+tuple([3]))[None]\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "#             frame = tf.numpy_function(crop_center_square, [frame], [tf.float32])\n",
    "\n",
    "            y, x = frame.shape[0:2]\n",
    "            min_dim = min(y, x)\n",
    "            start_x = (x // 2) - (min_dim // 2)\n",
    "            start_y = (y // 2) - (min_dim // 2)\n",
    "            frame = frame[start_y:start_y+min_dim,start_x:start_x+min_dim]\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = frame[:, :, [2, 1, 0]][None]\n",
    "            frames = np.vstack([frames, frame])\n",
    "\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "            \n",
    "        while len(frames) < min_frames or (len(frames) > min_frames and len(frames) < max_frames):\n",
    "            frames = np.vstack([frames, np.zeros(resize + tuple([3]))[None]])\n",
    "        \n",
    "    except:\n",
    "        print(frame)\n",
    "        \n",
    "    finally:\n",
    "        cap.release()\n",
    "    \n",
    "    frames = np.where((frames == -np.inf) | (frames == np.inf) | (pd.isna(frames)), 0.0, frames)\n",
    "#     frames[(frames == -np.inf) | (frames == np.inf) | (pd.isna(frames))] = 0.0\n",
    "    return (frames / 255.0).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_matrix(descriptions, embedding_dim, n_tokens):\n",
    "    \n",
    "    vectorizer = TextVectorization(max_tokens=n_tokens, output_sequence_length=25)\n",
    "    text_ds = Dataset.from_tensor_slices(descriptions).batch(16)\n",
    "    vectorizer.adapt(text_ds)\n",
    "\n",
    "    path_to_glove_file = \"utils/vocabulary/glove.6B.{}d.txt\".format(embedding_dim)\n",
    "\n",
    "    embeddings_index = {}\n",
    "    with open(path_to_glove_file, 'rb') as f:\n",
    "        data = [line.split(maxsplit=1) for line in f]\n",
    "    pretrained_embeddings = pd.DataFrame.from_records(data, columns=['word', 'coefs'])\n",
    "    pretrained_embeddings['word'] = pretrained_embeddings['word'].apply(lambda x: x.decode())\n",
    "    pretrained_embeddings['coefs'] = pretrained_embeddings['coefs'].apply(lambda x: np.fromstring(x, \"f\", sep=\" \"))\n",
    "    pretrained_embeddings = pretrained_embeddings.set_index('word', drop=True).iloc[:, 0]\n",
    "\n",
    "    voc = vectorizer.get_vocabulary()\n",
    "    num_tokens = len(voc) + 2\n",
    "    word_index = dict(zip(voc, range(len(voc))))\n",
    "    hits = 0\n",
    "    misses = 0\n",
    "\n",
    "    embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = pretrained_embeddings[word]\n",
    "            hits += 1\n",
    "        except KeyError:\n",
    "            misses += 1\n",
    "            pass\n",
    "    return vectorizer, embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['video'] = 'datasets/DiDeMo/' + train_data['video'] + '.mp4'\n",
    "paths = tf.data.Dataset.from_tensor_slices(train_data['video'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(tf.data.Dataset):\n",
    "    def _generator(paths, n_frames, size):\n",
    "        for path in paths:\n",
    "            yield load_video(path.decode(), n_frames, n_frames, resize=tuple(size))\n",
    "\n",
    "    def __new__(cls, paths, n_frames, size=(112,112)):\n",
    "        return tf.data.Dataset.from_generator(\n",
    "            cls._generator,\n",
    "            output_types=tf.dtypes.float32,\n",
    "            output_shapes=[n_frames] + list(size) + [3],\n",
    "            args=(paths, n_frames, size)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = VideoDataset(train_data['video'].compute().values, 900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoLayer(Layer):\n",
    "    def __init__(self):\n",
    "        super(VideoLayer, self).__init__()\n",
    "        self.conv3d_1 = Conv3D(32, kernel_size=7, activation='relu')\n",
    "        self.conv3d_2 = Conv3D(32, kernel_size=5, activation='relu')\n",
    "        self.conv3d_3 = Conv3D(32, kernel_size=3, activation='relu')\n",
    "        \n",
    "        self.maxpool3d_1 = MaxPool3D(pool_size=3)\n",
    "        self.maxpool3d_2 = MaxPool3D(pool_size=3)\n",
    "        self.maxpool3d_3 = MaxPool3D(pool_size=3)\n",
    "        \n",
    "        self.bn_1 = BatchNormalization()\n",
    "        self.flatten_1 = Flatten()\n",
    "        \n",
    "        self.dense_1 = Dense(units=128, activation='relu')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.conv3d_1(inputs)\n",
    "        x = self.maxpool3d_1(x)\n",
    "        x = self.conv3d_2(x)\n",
    "        x = self.maxpool3d_2(x)\n",
    "        x = self.conv3d_3(x)\n",
    "        x = self.maxpool3d_3(x)\n",
    "        x = self.bn_1(x)\n",
    "        x = self.flatten_1(x)\n",
    "        return self.dense_1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceLayer(Layer):\n",
    "    def __init__(self, n_tokens, embedding_dim, embedding_matrix):\n",
    "        super(SentenceLayer, self).__init__()\n",
    "        self.embedding_1 = Embedding(\n",
    "            n_tokens,\n",
    "            embedding_dim,\n",
    "            embeddings_initializer=initializers.Constant(embedding_matrix),\n",
    "            trainable=False\n",
    "        )\n",
    "        \n",
    "        self.bigru_1 = Bidirectional(GRU(64, return_sequences=True))\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.embedding_1(inputs)\n",
    "        return self.bigru_1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MomentVideo(Model):\n",
    "    def __init__(self, frame_rate, n_tokens, embedding_dim, embedding_matrix=None):\n",
    "        super(MomentVideo, self).__init__()\n",
    "        self.video_1 = VideoLayer()\n",
    "        self.sentence_1 = SentenceLayer(n_tokens, embedding_dim, embedding_matrix)\n",
    "        self.frame_rate = frame_rate\n",
    "    \n",
    "    \n",
    "    def cosine_similarity(self, tensor1, tensor2):\n",
    "        num = tf.linalg.matmul(expand_dims(tensor1, 1), expand_dims(tensor2, 1), transpose_a=True)\n",
    "        den = tf.norm(tensor1)*tf.norm(tensor2)\n",
    "        return num/(den+1e-15)\n",
    "    \n",
    "    \n",
    "    def matrix_cosine_similarity(self, tensor1, tensor2):\n",
    "        matrix = []\n",
    "        for i in range(tensor1.shape[0]):\n",
    "            row = []\n",
    "            for j in range(tensor2.shape[0]):\n",
    "                row.append(self.cosine_similarity(tensor1[i, :], tensor2[j, :]))\n",
    "            matrix.append(row)\n",
    "        return tf.stack(matrix)[:,:,0,0]\n",
    "    \n",
    "    \n",
    "    def similarity_between_repr_and_attend(self, tensor1, tensor2):\n",
    "        score = 0\n",
    "        for k in range(tensor1.shape[0]):\n",
    "            score += self.cosine_similarity(tensor1[k, :], tensor2[k, :])\n",
    "        return score / tensor1.shape[0]\n",
    "    \n",
    "    \n",
    "    def get_scores(self, video_repr_tensor, sentence_repr_tensor):\n",
    "        coattention_matrix = self.matrix_cosine_similarity(video_repr_tensor, sentence_repr_tensor)\n",
    "        \n",
    "        normalized_sentence = tf.nn.softmax(coattention_matrix, axis=1)\n",
    "        normalized_video = tf.nn.softmax(coattention_matrix, axis=0)\n",
    "            \n",
    "        matrix = []\n",
    "        for i in range(video_repr_tensor.shape[0]):\n",
    "            row_sum = np.zeros((128))\n",
    "            for j in range(sentence_repr_tensor.shape[0]):\n",
    "                row_sum += normalized_sentence[i][j] * sentence_repr_tensor[j, :]\n",
    "            matrix.append(row_sum)\n",
    "            \n",
    "        sentence_attention = tf.stack(matrix)\n",
    "    \n",
    "        matrix = []\n",
    "        for j in range(sentence_repr_tensor.shape[0]):\n",
    "            row_sum = np.zeros((128))\n",
    "            for i in range(video_repr_tensor.shape[0]):\n",
    "                row_sum += normalized_video[i][j] * video_repr_tensor[i, :]\n",
    "            matrix.append(row_sum)\n",
    "            \n",
    "        video_attention = tf.stack(matrix)\n",
    "        \n",
    "        score_video = self.similarity_between_repr_and_attend(video_repr_tensor, sentence_attention)\n",
    "        score_sentence = self.similarity_between_repr_and_attend(sentence_repr_tensor, video_attention)\n",
    "        \n",
    "        return score_video, score_sentence\n",
    "    \n",
    "    \n",
    "    def call(self, videos, sentences):\n",
    "        '''\n",
    "            Parameters:\n",
    "                videos (batch) - the raw videos of the dataset\n",
    "                sentences (batch) - the sentences of the dataset\n",
    "            \n",
    "            Return:\n",
    "                matrix_score - matrix that represents the scores of the each video wrt each sentence\n",
    "        '''\n",
    "\n",
    "        \n",
    "        #extract the features\n",
    "        video_repr = []\n",
    "        for last_frame in range(videos.shape[1] // self.frame_rate):\n",
    "            video_repr.append(self.video_1(videos[:, last_frame*self.frame_rate:(last_frame+1)*self.frame_rate]))\n",
    "        videos_repr = tf.stack(video_repr, axis=1)\n",
    "        sentences_repr = self.sentence_1(sentences)\n",
    "        \n",
    "        n_batch = videos.shape[0]\n",
    "        \n",
    "        scores_video = np.empty([n_batch, n_batch])\n",
    "        scores_sentence = np.empty([n_batch, n_batch])\n",
    "        \n",
    "        for i in range(n_batch):\n",
    "            for j in range(n_batch):\n",
    "                scores_video[i][j], scores_sentence[j][i] = self.get_scores(videos_repr[i], sentences_repr[j])\n",
    "                \n",
    "        return scores_video, scores_sentence\n",
    "    \n",
    "    \n",
    "#     def train_step(self, data):\n",
    "#         # Unpack the data. Its structure depends on your model and\n",
    "#         # on what you pass to `fit()`.\n",
    "#         videos, sentences, y = data\n",
    "        \n",
    "#         video_repr = []\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             y_pred = self(x, training=True)  # Forward pass\n",
    "#             # Compute the loss value\n",
    "#             # (the loss function is configured in `compile()`)\n",
    "#             loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "\n",
    "#         # Compute gradients\n",
    "#         trainable_vars = self.trainable_variables\n",
    "#         gradients = tape.gradient(loss, trainable_vars)\n",
    "#         # Update weights\n",
    "#         self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "#         # Update metrics (includes the metric that tracks the loss)\n",
    "#         self.compiled_metrics.update_state(y, y_pred)\n",
    "#         # Return a dict mapping metric names to current value\n",
    "#         return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def margin_based_ranking_loss(scores_video, scores_sentence, margin=1, top_k=2):\n",
    "    \n",
    "#     score_true_video = tf.linalg.tensor_diag_part(scores_video)\n",
    "    shape_negative = list(scores_video.shape)\n",
    "    shape_negative[-1] = shape_negative[-1] - 1\n",
    "\n",
    "    score_true_videos = scores_video.diagonal()\n",
    "    scores_negatives_videos = np.sort(scores_video[scores_video != scores_video.T].reshape(shape_negative), axis=1)[:, ::-1][:, :top_k]\n",
    "    \n",
    "    video_loss = 1 - np.tile(score_true_videos.reshape(-1, 1), scores_negatives_videos.shape[1]) + scores_negatives_videos\n",
    "    video_loss = np.where(video_loss < 0, 0, video_loss)\n",
    "    video_loss = np.sum(video_loss, axis=1)\n",
    "    \n",
    "    \n",
    "    shape_negative = list(scores_sentence.shape)\n",
    "    shape_negative[-1] = shape_negative[-1] - 1\n",
    "\n",
    "    score_true_sentences = scores_sentence.diagonal()\n",
    "    scores_negatives_sentences = np.sort(scores_sentence[scores_sentence != scores_sentence.T].reshape(shape_negative), axis=1)[:, ::-1][:, :top_k]\n",
    "\n",
    "    sentence_loss = 1 - np.tile(score_true_sentences.reshape(-1, 1), scores_negatives_sentences.shape[1]) + scores_negatives_sentences\n",
    "    sentence_loss = np.where(sentence_loss < 0, 0, sentence_loss)\n",
    "    sentence_loss = np.sum(sentence_loss, axis=1)\n",
    "    \n",
    "    return video_loss + sentence_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "vectorizer, embedding_matrix = get_embedding_matrix(train_data['description'], embedding_dim, 20000)\n",
    "iter_videos = iter(videos.batch(4))\n",
    "descriptions = vectorizer(train_data['description'].compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_video = iter_videos.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('cpu:0'):\n",
    "    moment = MomentVideo(75, len(embedding_matrix), embedding_dim, embedding_matrix)\n",
    "    scores_video, scores_sentence = moment(batch_video, descriptions[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08091357, 0.11735822, 0.12237162, 0.13858634],\n",
       "       [0.08166637, 0.12108622, 0.12046162, 0.14008217],\n",
       "       [0.07299844, 0.10532001, 0.10973985, 0.11165836],\n",
       "       [0.08955354, 0.12799676, 0.12595092, 0.13003561]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04544296, 0.05315233, 0.05486396, 0.06009117],\n",
       "       [0.07006237, 0.07351372, 0.08248698, 0.08085805],\n",
       "       [0.06270567, 0.06977195, 0.07429998, 0.07614164],\n",
       "       [0.05711501, 0.06609467, 0.05781157, 0.06316324]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Exploratory Analysis over Video Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_segments</th>\n",
       "      <th>description</th>\n",
       "      <th>dl_link</th>\n",
       "      <th>times</th>\n",
       "      <th>video</th>\n",
       "      <th>annotation_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>a brown rat goes into someone's hand then onto...</td>\n",
       "      <td>https://www.flickr.com/video_download.gne?id=2...</td>\n",
       "      <td>[[2, 2], [2, 2], [2, 2], [2, 2]]</td>\n",
       "      <td>54322086@N00_2408598493_274c77d26a.avi</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>an orange kitten is sitting then gets up and w...</td>\n",
       "      <td>https://www.flickr.com/video_download.gne?id=2...</td>\n",
       "      <td>[[5, 5], [5, 5], [5, 5], [5, 5]]</td>\n",
       "      <td>99051133@N00_2502628368_d14bd317de.mov</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>a person walks outside and then back in</td>\n",
       "      <td>https://www.flickr.com/video_download.gne?id=5...</td>\n",
       "      <td>[[3, 3], [3, 3], [3, 3], [3, 4]]</td>\n",
       "      <td>67801451@N00_5358663022_243bd90fbc.mov</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>the guards spin around 180 degrees</td>\n",
       "      <td>https://www.flickr.com/video_download.gne?id=4...</td>\n",
       "      <td>[[4, 4], [4, 5], [3, 4], [3, 4]]</td>\n",
       "      <td>64379474@N00_4479342537_7b5a3d3f1d.avi</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>the plane flies off the roof</td>\n",
       "      <td>https://www.flickr.com/video_download.gne?id=9...</td>\n",
       "      <td>[[5, 5], [4, 5], [4, 5], [4, 5]]</td>\n",
       "      <td>63122283@N06_9978694646_e72011157f.mov</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33000</th>\n",
       "      <td>6</td>\n",
       "      <td>man holds his hand up to feed the birds for th...</td>\n",
       "      <td>https://www.flickr.com/video_download.gne?id=3...</td>\n",
       "      <td>[[1, 1], [1, 1], [1, 1], [1, 1]]</td>\n",
       "      <td>65977087@N00_3751407080_6bda50e6df.mov</td>\n",
       "      <td>64632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33001</th>\n",
       "      <td>6</td>\n",
       "      <td>someone walks across bottom screen</td>\n",
       "      <td>https://www.flickr.com/video_download.gne?id=7...</td>\n",
       "      <td>[[0, 1], [0, 0], [2, 2], [0, 0]]</td>\n",
       "      <td>89333651@N00_7564648598_776b587a55.mp4</td>\n",
       "      <td>64637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33002</th>\n",
       "      <td>6</td>\n",
       "      <td>first time we can see the musicians</td>\n",
       "      <td>https://www.flickr.com/video_download.gne?id=4...</td>\n",
       "      <td>[[2, 2], [2, 2], [2, 2], [2, 2]]</td>\n",
       "      <td>77118917@N00_4606529468_6ba24fe0c9.avi</td>\n",
       "      <td>64638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33003</th>\n",
       "      <td>6</td>\n",
       "      <td>the pitcher's mound comes into view</td>\n",
       "      <td>https://www.flickr.com/video_download.gne?id=4...</td>\n",
       "      <td>[[3, 3], [3, 3], [3, 3], [3, 3]]</td>\n",
       "      <td>72426516@N00_4868053620_46b99ec39f.mov</td>\n",
       "      <td>64641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33004</th>\n",
       "      <td>6</td>\n",
       "      <td>the dancing woman spins for the last time</td>\n",
       "      <td>https://www.flickr.com/video_download.gne?id=8...</td>\n",
       "      <td>[[2, 2], [2, 2], [0, 0], [2, 2]]</td>\n",
       "      <td>72511036@N00_8457819704_37bed14ae1.</td>\n",
       "      <td>64644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33005 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       num_segments                                        description  \\\n",
       "0                 6  a brown rat goes into someone's hand then onto...   \n",
       "1                 6  an orange kitten is sitting then gets up and w...   \n",
       "2                 6            a person walks outside and then back in   \n",
       "3                 6                 the guards spin around 180 degrees   \n",
       "4                 6                       the plane flies off the roof   \n",
       "...             ...                                                ...   \n",
       "33000             6  man holds his hand up to feed the birds for th...   \n",
       "33001             6                 someone walks across bottom screen   \n",
       "33002             6                first time we can see the musicians   \n",
       "33003             6                the pitcher's mound comes into view   \n",
       "33004             6          the dancing woman spins for the last time   \n",
       "\n",
       "                                                 dl_link  \\\n",
       "0      https://www.flickr.com/video_download.gne?id=2...   \n",
       "1      https://www.flickr.com/video_download.gne?id=2...   \n",
       "2      https://www.flickr.com/video_download.gne?id=5...   \n",
       "3      https://www.flickr.com/video_download.gne?id=4...   \n",
       "4      https://www.flickr.com/video_download.gne?id=9...   \n",
       "...                                                  ...   \n",
       "33000  https://www.flickr.com/video_download.gne?id=3...   \n",
       "33001  https://www.flickr.com/video_download.gne?id=7...   \n",
       "33002  https://www.flickr.com/video_download.gne?id=4...   \n",
       "33003  https://www.flickr.com/video_download.gne?id=4...   \n",
       "33004  https://www.flickr.com/video_download.gne?id=8...   \n",
       "\n",
       "                                  times  \\\n",
       "0      [[2, 2], [2, 2], [2, 2], [2, 2]]   \n",
       "1      [[5, 5], [5, 5], [5, 5], [5, 5]]   \n",
       "2      [[3, 3], [3, 3], [3, 3], [3, 4]]   \n",
       "3      [[4, 4], [4, 5], [3, 4], [3, 4]]   \n",
       "4      [[5, 5], [4, 5], [4, 5], [4, 5]]   \n",
       "...                                 ...   \n",
       "33000  [[1, 1], [1, 1], [1, 1], [1, 1]]   \n",
       "33001  [[0, 1], [0, 0], [2, 2], [0, 0]]   \n",
       "33002  [[2, 2], [2, 2], [2, 2], [2, 2]]   \n",
       "33003  [[3, 3], [3, 3], [3, 3], [3, 3]]   \n",
       "33004  [[2, 2], [2, 2], [0, 0], [2, 2]]   \n",
       "\n",
       "                                        video  annotation_id  \n",
       "0      54322086@N00_2408598493_274c77d26a.avi              2  \n",
       "1      99051133@N00_2502628368_d14bd317de.mov              3  \n",
       "2      67801451@N00_5358663022_243bd90fbc.mov              4  \n",
       "3      64379474@N00_4479342537_7b5a3d3f1d.avi              5  \n",
       "4      63122283@N06_9978694646_e72011157f.mov              7  \n",
       "...                                       ...            ...  \n",
       "33000  65977087@N00_3751407080_6bda50e6df.mov          64632  \n",
       "33001  89333651@N00_7564648598_776b587a55.mp4          64637  \n",
       "33002  77118917@N00_4606529468_6ba24fe0c9.avi          64638  \n",
       "33003  72426516@N00_4868053620_46b99ec39f.mov          64641  \n",
       "33004     72511036@N00_8457819704_37bed14ae1.          64644  \n",
       "\n",
       "[33005 rows x 6 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.Series(train_data).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127.99600000000646"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 150)\n",
    "df['description'].str.len().quantile(0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso significa que em 99.9% dos casos, as descrições não passaram de 128 caractereces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.0"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['description'].str.split().str.len().quantile(0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso significa que em 99.9% dos casos, as descrições não passaram de 25 palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = df['description'].str.split()\n",
    "word_counts = pd.value_counts(words.apply(pd.Series).stack())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the          24341\n",
       "a            10974\n",
       "in            8262\n",
       "to            5668\n",
       "man           4637\n",
       "             ...  \n",
       "Green,           1\n",
       "snatched         1\n",
       "filmer.          1\n",
       "harnesses        1\n",
       "master.          1\n",
       "Length: 9260, dtype: int64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claramente, as palavras mais citadas são os artigos como: 'the', 'a', 'in', 'to'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=5.0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.norm([3.0, 4.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
