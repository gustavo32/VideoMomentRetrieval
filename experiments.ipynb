{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import configurations, utils, base, image_sequence, sentence\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 12\n",
      "clip_length = 150\n",
      "fps = 30\n",
      "include_audio_features = False\n",
      "n_features = 128\n",
      "sentence\n",
      "  --- embedding_file = utils/vocabulary/glove.6B.50d.txt\n",
      "  --- embeddings_dim = 50\n",
      "  --- embeddings_folder = utils/vocabulary/\n",
      "  --- n_tokens = 20000\n",
      "test_info_path = data/test_data.json\n",
      "train_info_path = data/train_data.json\n",
      "valid_info_path = data/val_data.json\n",
      "video\n",
      "  --- features_folder = rgb_vgg_fc7_features/\n",
      "  --- files_pattern = datasets/DiDeMo/{}.mp4\n",
      "  --- max_frames = 900\n",
      "  --- n_extracted_features = 400\n",
      "  --- n_splits = 6\n",
      "  --- size = (224, 224)\n"
     ]
    }
   ],
   "source": [
    "configs = configurations.DatasetConfigs()\n",
    "configs.n_features = 128\n",
    "configs.batch_size = 12\n",
    "configs.video.features_folder = \"rgb_vgg_fc7_features/\"\n",
    "configs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with configs:\n",
    "    train_ds, valid_ds, test_ds, embedding_matrix = base.preprocess_datasets()\n",
    "\n",
    "    video_layer = image_sequence.VideoLayer()\n",
    "    sentence_layer = sentence.SentenceLayer(embedding_matrix)\n",
    "    #     audio_layer = get_audio_layer()\n",
    "\n",
    "    moment = base.MomentVideo(video_layer, sentence_layer, configs.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.set_soft_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "moment.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348/348 [==============================] - 67s 194ms/step - mIOU: 0.2370 - loss: 24.1265\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moment.evaluate(valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "600/600 [==============================] - 320s 534ms/step - loss: 24.1866 - val_mIOU: 0.2083 - val_loss: 24.1444\n",
      "Epoch 2/10\n",
      "600/600 [==============================] - 331s 551ms/step - loss: 24.0459 - val_mIOU: 0.2083 - val_loss: 24.1248\n",
      "Epoch 3/10\n",
      "600/600 [==============================] - 322s 536ms/step - loss: 24.0234 - val_mIOU: 0.2083 - val_loss: 24.1028\n",
      "Epoch 4/10\n",
      "600/600 [==============================] - 313s 521ms/step - loss: 24.0169 - val_mIOU: 0.2083 - val_loss: 24.1060\n",
      "Epoch 5/10\n",
      " 18/600 [..............................] - ETA: 3:57 - loss: 24.0013"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-373c7c3f0600>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"GPU:0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mmoment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid_ds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m600\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\luis_\\documents\\tcc\\tcc_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\luis_\\documents\\tcc\\tcc_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\luis_\\documents\\tcc\\tcc_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\luis_\\documents\\tcc\\tcc_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\luis_\\documents\\tcc\\tcc_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\luis_\\documents\\tcc\\tcc_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\luis_\\documents\\tcc\\tcc_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\luis_\\documents\\tcc\\tcc_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mc:\\users\\luis_\\documents\\tcc\\tcc_env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.device(\"GPU:0\"):\n",
    "    moment.fit(train_ds.repeat(), epochs=10, validation_data=valid_ds, steps_per_epoch=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metrics import mIOU, prediction_by_moving_average, intersection_over_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = iter(test_ds).get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_15 = 0\n",
    "mean_7 = 0\n",
    "mean_30 = 0\n",
    "count = 0\n",
    "for data in iter(test_ds):\n",
    "    videos, sentences, y_true = data\n",
    "\n",
    "    videos_repr = video_layer(videos)\n",
    "    sentences_repr = sentence_layer(sentences)\n",
    "\n",
    "    scores_videos = []\n",
    "    for i in range(configs.batch_size):\n",
    "        coattention_matrix = matrix_cosine_similarity(videos_repr[i], sentences_repr[i])\n",
    "        scores_video = get_video_score(videos_repr[i], sentences_repr[i], coattention_matrix)\n",
    "        scores_videos.append(scores_video)\n",
    "\n",
    "    scores_videos = tf.stack(scores_videos)\n",
    "    miou_score = tf.numpy_function(mIOU, [scores_videos, y_true, 7], tf.float64)\n",
    "    mean_7 += miou_score\n",
    "    miou_score = tf.numpy_function(mIOU, [scores_videos, y_true, 15], tf.float64)\n",
    "    mean_15 += miou_score\n",
    "    miou_score = tf.numpy_function(mIOU, [scores_videos, y_true, 30], tf.float64)\n",
    "    mean_30 += miou_score\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_15 = 0\n",
    "mean_7 = 0\n",
    "mean_30 = 0\n",
    "count = 0\n",
    "for data in iter(train_ds):\n",
    "    videos, sentences, y_true = data\n",
    "\n",
    "    videos_repr = moment.video_1(videos)\n",
    "    sentences_repr = moment.sentence_1(sentences)\n",
    "\n",
    "    scores_videos = []\n",
    "    for i in range(moment.batch_size):\n",
    "        #coattention_matrix = self.matrix_cosine_similarity(videos_repr[i], sentences_repr[i])\n",
    "        scores_video = moment.get_video_score(videos_repr[i], sentences_repr[i])\n",
    "        scores_videos.append(scores_video)\n",
    "\n",
    "    scores_videos = tf.stack(scores_videos)\n",
    "    miou_score = tf.numpy_function(mIOU, [scores_videos, y_true, 7], tf.float64)\n",
    "    mean_7 += miou_score\n",
    "    miou_score = tf.numpy_function(mIOU, [scores_videos, y_true, 15], tf.float64)\n",
    "    mean_15 += miou_score\n",
    "    miou_score = tf.numpy_function(mIOU, [scores_videos, y_true, 30], tf.float64)\n",
    "    mean_30 += miou_score\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.19779595959595933, shape=(), dtype=float64)\n",
      "tf.Tensor(0.19475303030302968, shape=(), dtype=float64)\n",
      "tf.Tensor(0.2000580808080813, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "print(mean_7/count)\n",
    "print(mean_15/count)\n",
    "print(mean_30/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_proposals(scores):\n",
    "    n_proposals = scores.shape[1] // 25\n",
    "    proposals = list(product(range(n_proposals), range(n_proposals)))\n",
    "    y_pred = []\n",
    "    for score in scores:\n",
    "        max_score = 0\n",
    "        proposal = [0, 0]\n",
    "        for proposal in proposals:\n",
    "            if proposal[1] >= proposal[0]:\n",
    "                final_score = tf.reduce_mean(score[proposal[0]*25:(proposal[1]+1)*25])\n",
    "                if final_score > max_score:\n",
    "                    max_score = final_score\n",
    "                    max_proposal = proposal\n",
    "                    \n",
    "        y_pred.append(max_proposal)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_scores = 0\n",
    "count = 0\n",
    "for data in iter(test_ds):\n",
    "    videos, sentences, y_true = data\n",
    "\n",
    "    videos_repr = moment.video_1(videos)\n",
    "    sentences_repr = moment.sentence_1(sentences)\n",
    "\n",
    "    scores_videos = []\n",
    "    for i in range(moment.batch_size):\n",
    "        coattention_matrix = moment.matrix_cosine_similarity(videos_repr[i], sentences_repr[i])\n",
    "        scores_video = moment.get_video_score(videos_repr[i], sentences_repr[i], coattention_matrix)\n",
    "        scores_videos.append(scores_video)\n",
    "\n",
    "    scores_videos = tf.stack(scores_videos)\n",
    "    \n",
    "    y_pred = tf.cast(tf.convert_to_tensor(generate_proposals(scores_videos)), tf.int64)\n",
    "    scores = np.apply_along_axis(\n",
    "        intersection_over_union, 1, np.concatenate([y_true, y_pred], axis=-1))\n",
    "    total_scores += tf.reduce_sum(scores)\n",
    "    count+=12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.16770729684908797, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "print(total_scores/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def xattn_score_t2i(images, captions, cap_lens, opt):\n",
    "    \"\"\"\n",
    "    Images: (n_image, n_regions, d) matrix of images\n",
    "    Captions: (n_caption, max_n_word, d) matrix of captions\n",
    "    CapLens: (n_caption) array of caption lengths\n",
    "    \"\"\"\n",
    "    similarities = []\n",
    "    n_image = images.size(0)\n",
    "    n_caption = captions.size(0)\n",
    "    for i in range(n_caption):\n",
    "        # Get the i-th text description\n",
    "        n_word = cap_lens[i]\n",
    "        cap_i = captions[i, :n_word, :].unsqueeze(0).contiguous()\n",
    "        # --> (n_image, n_word, d)\n",
    "        cap_i_expand = cap_i.repeat(n_image, 1, 1)\n",
    "        \"\"\"\n",
    "            word(query): (n_image, n_word, d)\n",
    "            image(context): (n_image, n_regions, d)\n",
    "            weiContext: (n_image, n_word, d)\n",
    "            attn: (n_image, n_region, n_word)\n",
    "        \"\"\"\n",
    "        weiContext, attn = func_attention(cap_i_expand, images, opt, smooth=opt.lambda_softmax)\n",
    "        print()\n",
    "        cap_i_expand = cap_i_expand.contiguous()\n",
    "        weiContext = weiContext.contiguous()\n",
    "        # (n_image, n_word)\n",
    "        row_sim = cosine_similarity(cap_i_expand, weiContext, dim=2)\n",
    "        if opt.agg_func == 'LogSumExp':\n",
    "            row_sim.mul_(opt.lambda_lse).exp_()\n",
    "            row_sim = row_sim.sum(dim=1, keepdim=True)\n",
    "            row_sim = torch.log(row_sim)/opt.lambda_lse\n",
    "        elif opt.agg_func == 'Max':\n",
    "            row_sim = row_sim.max(dim=1, keepdim=True)[0]\n",
    "        elif opt.agg_func == 'Sum':\n",
    "            row_sim = row_sim.sum(dim=1, keepdim=True)\n",
    "        elif opt.agg_func == 'Mean':\n",
    "            row_sim = row_sim.mean(dim=1, keepdim=True)\n",
    "        else:\n",
    "            raise ValueError(\"unknown aggfunc: {}\".format(opt.agg_func))\n",
    "        similarities.append(row_sim)\n",
    "\n",
    "    # (n_image, n_caption)\n",
    "    similarities = torch.cat(similarities, 1)\n",
    "    \n",
    "    return similarities\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Compute contrastive loss\n",
    "    \"\"\"\n",
    "    def __init__(self, margin=0, max_violation=False):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.max_violation = max_violation\n",
    "\n",
    "    def forward(self, scores):\n",
    "        # compute image-sentence score matrix\n",
    "#         if self.opt.cross_attn == 't2i':\n",
    "#             scores = xattn_score_t2i(im, s, s_l, self.opt)\n",
    "#         elif self.opt.cross_attn == 'i2t':\n",
    "#             scores = xattn_score_i2t(im, s, s_l, self.opt)\n",
    "#         else:\n",
    "#             raise ValueError(\"unknown first norm type:\", opt.raw_feature_norm)\n",
    "        diagonal = scores.diag().view(scores.size(0), 1)\n",
    "        d1 = diagonal.expand_as(scores)\n",
    "        d2 = diagonal.t().expand_as(scores)\n",
    "\n",
    "        # compare every diagonal score to scores in its column\n",
    "        # caption retrieval\n",
    "        cost_s = (1 + scores - d1).clamp(min=0)\n",
    "        # compare every diagonal score to scores in its row\n",
    "        # image retrieval\n",
    "        cost_im = (1 + scores - d2).clamp(min=0)\n",
    "\n",
    "        # clear diagonals\n",
    "        mask = torch.eye(scores.size(0)) > .5\n",
    "        I = Variable(mask)\n",
    "        cost_s = cost_s.masked_fill_(I, 0)\n",
    "        cost_im = cost_im.masked_fill_(I, 0)\n",
    "\n",
    "        # keep the maximum violating negative for each query\n",
    "        if self.max_violation:\n",
    "            cost_s = cost_s.max(1)[0]\n",
    "            cost_im = cost_im.max(0)[0]\n",
    "        return cost_s.sum() + cost_im.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranking_loss(scores, margin, top_k=8):\n",
    "    scores_positives = tf.linalg.diag_part(scores)\n",
    "\n",
    "    shape_negative = list(scores.shape)\n",
    "    bool_matrix = tf.ones(shape_negative)\n",
    "    bool_matrix = tf.linalg.set_diag(bool_matrix, tf.zeros(shape_negative[0])) == 1\n",
    "    shape_negative[-1] = shape_negative[-1] - 1\n",
    "\n",
    "    scores_negatives = tf.reshape(scores[bool_matrix], shape_negative)\n",
    "    top_k_scores_negatives = tf.sort(scores_negatives, axis=1, direction=\"DESCENDING\")[:, 0]\n",
    "\n",
    "    loss = 1 - scores_positives + top_k_scores_negatives\n",
    "    loss = tf.where(loss < 0.0, 0.0, loss)\n",
    "\n",
    "    return tf.reduce_sum(loss)\n",
    "\n",
    "\n",
    "def margin_based_ranking_loss(scores_videos, scores_sentences, margin=1, top_k=8):\n",
    "    video_loss = get_ranking_loss(scores_videos, margin, top_k)\n",
    "    sentence_loss = get_ranking_loss(scores_sentences, margin, top_k)\n",
    "    \n",
    "    return video_loss + sentence_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_videos, scores_sentences = moment.call((data[0], data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(262.3871)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ContrastiveLoss()(torch.from_numpy(scores_videos.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=12.396402>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ranking_loss(scores_videos, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.from_numpy(scores_videos.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = iter(test_ds).get_next()\n",
    "videos_repr = moment.video_1(data[0])\n",
    "sentences_repr = moment.sentence_1(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = torch.from_numpy(videos_repr.numpy())\n",
    "captions = torch.from_numpy(sentences_repr.numpy())\n",
    "cap_lens = torch.from_numpy(lengths.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_attention(query, context, smooth, eps=1e-8):\n",
    "    \"\"\"\n",
    "    query: (n_context, queryL, d)\n",
    "    context: (n_context, sourceL, d)\n",
    "    \"\"\"\n",
    "    batch_size_q, queryL = query.size(0), query.size(1)\n",
    "    batch_size, sourceL = context.size(0), context.size(1)\n",
    "\n",
    "\n",
    "    # Get attention\n",
    "    # --> (batch, d, queryL)\n",
    "    queryT = torch.transpose(query, 1, 2)\n",
    "\n",
    "    # (batch, sourceL, d)(batch, d, queryL)\n",
    "    # --> (batch, sourceL, queryL)\n",
    "    attn = torch.bmm(context, queryT)\n",
    "#     if opt.raw_feature_norm == \"softmax\":\n",
    "#         # --> (batch*sourceL, queryL)\n",
    "#         attn = attn.view(batch_size*sourceL, queryL)\n",
    "#         attn = nn.Softmax()(attn)\n",
    "#         # --> (batch, sourceL, queryL)\n",
    "#         attn = attn.view(batch_size, sourceL, queryL)\n",
    "#     elif opt.raw_feature_norm == \"l2norm\":\n",
    "#         attn = l2norm(attn, 2)\n",
    "#     elif opt.raw_feature_norm == \"clipped_l2norm\": #usado normalmente\n",
    "#         attn = nn.LeakyReLU(0.1)(attn)\n",
    "#         attn = l2norm(attn, 2)\n",
    "#     elif opt.raw_feature_norm == \"l1norm\":\n",
    "#         attn = l1norm_d(attn, 2)\n",
    "#     elif opt.raw_feature_norm == \"clipped_l1norm\":\n",
    "#         attn = nn.LeakyReLU(0.1)(attn)\n",
    "#         attn = l1norm_d(attn, 2)\n",
    "#     elif opt.raw_feature_norm == \"clipped\":\n",
    "#         attn = nn.LeakyReLU(0.1)(attn)\n",
    "#     elif opt.raw_feature_norm == \"no_norm\":\n",
    "#         pass\n",
    "#     else:\n",
    "#         raise ValueError(\"unknown first norm type:\", opt.raw_feature_norm)\n",
    "    # --> (batch, queryL, sourceL)\n",
    "    attn = torch.transpose(attn, 1, 2).contiguous()\n",
    "    # --> (batch*queryL, sourceL)\n",
    "    attn = attn.view(batch_size*queryL, sourceL)\n",
    "    attn = nn.Softmax()(attn*smooth)\n",
    "    # --> (batch, queryL, sourceL)\n",
    "    attn = attn.view(batch_size, queryL, sourceL)\n",
    "    # --> (batch, sourceL, queryL)\n",
    "    attnT = torch.transpose(attn, 1, 2).contiguous()\n",
    "\n",
    "    # --> (batch, d, sourceL)\n",
    "    contextT = torch.transpose(context, 1, 2)\n",
    "    # (batch x d x sourceL)(batch x sourceL x queryL)\n",
    "    # --> (batch, d, queryL)\n",
    "    weightedContext = torch.bmm(contextT, attnT)\n",
    "    # --> (batch, queryL, d)\n",
    "    weightedContext = torch.transpose(weightedContext, 1, 2)\n",
    "\n",
    "    return weightedContext, attnT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.9431, 1.7703, 2.1892, 2.1531, 2.2121, 1.9591, 2.4017, 1.4632, 1.9711,\n",
      "         2.7199, 1.9371, 1.7407],\n",
      "        [2.0120, 1.8523, 2.2039, 2.2174, 2.3001, 1.9903, 2.4443, 1.4378, 2.0332,\n",
      "         2.7731, 2.0203, 1.8760],\n",
      "        [1.9344, 1.8250, 2.1832, 2.1645, 2.1897, 1.9731, 2.3717, 1.4712, 1.9309,\n",
      "         2.6850, 1.9666, 1.7853],\n",
      "        [1.9939, 1.8438, 2.2456, 2.2681, 2.2908, 2.0080, 2.4695, 1.4769, 2.0152,\n",
      "         2.7646, 2.0093, 1.8757],\n",
      "        [2.0942, 1.9954, 2.3747, 2.4108, 2.3422, 2.1091, 2.5893, 1.6596, 2.1577,\n",
      "         2.8742, 2.1335, 1.9484],\n",
      "        [2.0105, 1.7982, 2.2342, 2.2858, 2.2660, 1.9675, 2.4553, 1.4045, 1.9990,\n",
      "         2.7719, 1.9954, 1.8556],\n",
      "        [1.9837, 1.8423, 2.2284, 2.2312, 2.3162, 2.0031, 2.4751, 1.4330, 2.0578,\n",
      "         2.8265, 2.0234, 1.8789],\n",
      "        [1.9276, 1.7240, 2.1259, 2.1465, 2.1960, 1.8922, 2.3520, 1.3686, 1.8991,\n",
      "         2.6574, 1.9461, 1.7680],\n",
      "        [1.8869, 1.7628, 2.1234, 2.1250, 2.2119, 1.8845, 2.3515, 1.3353, 1.9227,\n",
      "         2.6882, 1.9143, 1.7407],\n",
      "        [1.8967, 1.7782, 2.1405, 2.1340, 2.2012, 1.9067, 2.3888, 1.4105, 1.9177,\n",
      "         2.7056, 1.9065, 1.7566],\n",
      "        [1.9933, 1.7811, 2.1984, 2.2029, 2.2231, 1.9735, 2.4558, 1.4500, 1.9931,\n",
      "         2.7402, 1.9934, 1.8083],\n",
      "        [2.0030, 1.8175, 2.2155, 2.1897, 2.2750, 1.9822, 2.4341, 1.3653, 2.0304,\n",
      "         2.7579, 1.9861, 1.7857]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Images: (n_image, n_regions, d) matrix of images\n",
    "Captions: (n_caption, max_n_word, d) matrix of captions\n",
    "CapLens: (n_caption) array of caption lengths\n",
    "\"\"\"\n",
    "similarities = []\n",
    "n_image = images.size(0)\n",
    "n_caption = captions.size(0)\n",
    "for i in range(n_caption):\n",
    "    # Get the i-th text description\n",
    "    n_word = cap_lens[i]\n",
    "    cap_i = captions[i, :n_word, :].unsqueeze(0).contiguous()\n",
    "    # --> (n_image, n_word, d)\n",
    "    cap_i_expand = cap_i.repeat(n_image, 1, 1)\n",
    "    \"\"\"\n",
    "        word(query): (n_image, n_word, d)\n",
    "        image(context): (n_image, n_regions, d)\n",
    "        weiContext: (n_image, n_word, d)\n",
    "        attn: (n_image, n_region, n_word)\n",
    "    \"\"\"\n",
    "    weiContext, attn = func_attention(cap_i_expand, images, smooth=1.)\n",
    "    cap_i_expand = cap_i_expand.contiguous()\n",
    "    weiContext = weiContext.contiguous()\n",
    "    # (n_image, n_word)\n",
    "    row_sim = cosine_similarity_pytorch(cap_i_expand, weiContext, dim=2)\n",
    "#     print(row_sim)\n",
    "    row_sim.mul_(1.).exp_()\n",
    "    row_sim = row_sim.sum(dim=1, keepdim=True)\n",
    "    row_sim = torch.log(row_sim)/1.\n",
    "#     elif opt.agg_func == 'Max':\n",
    "#         row_sim = row_sim.max(dim=1, keepdim=True)[0]\n",
    "#     elif opt.agg_func == 'Sum':\n",
    "#         row_sim = row_sim.sum(dim=1, keepdim=True)\n",
    "#     elif opt.agg_func == 'Mean':\n",
    "#         row_sim = row_sim.mean(dim=1, keepdim=True)\n",
    "#     else:\n",
    "#         raise ValueError(\"unknown aggfunc: {}\".format(opt.agg_func))\n",
    "    similarities.append(row_sim)\n",
    "\n",
    "# (n_image, n_caption)\n",
    "similarities = torch.cat(similarities, 1)\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = torch.transpose(attn, 1, 2).contiguous()\n",
    "# --> (batch*queryL, sourceL)\n",
    "attn = attn.view(batch_size*queryL, sourceL)\n",
    "attn = nn.Softmax()(attn*smooth)\n",
    "# --> (batch, queryL, sourceL)\n",
    "attn = attn.view(batch_size, queryL, sourceL)\n",
    "# --> (batch, sourceL, queryL)\n",
    "attnT = torch.transpose(attn, 1, 2).contiguous()\n",
    "\n",
    "# --> (batch, d, sourceL)\n",
    "contextT = torch.transpose(context, 1, 2)\n",
    "# (batch x d x sourceL)(batch x sourceL x queryL)\n",
    "# --> (batch, d, queryL)\n",
    "weightedContext = torch.bmm(contextT, attnT)\n",
    "# --> (batch, queryL, d)\n",
    "weightedContext = torch.transpose(weightedContext, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(tensor1, tensor2, axis=2):\n",
    "    num = tf.reduce_sum(tensor1 * tensor2, axis=axis)\n",
    "    den = tf.norm(tensor1, axis=axis) * tf.norm(tensor2, axis=axis)\n",
    "    return (num/(den+1e-15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_pytorch(x1, x2, dim=1, eps=1e-8):\n",
    "    \"\"\"Returns cosine similarity between x1 and x2, computed along dim.\"\"\"\n",
    "    w12 = torch.sum(x1 * x2, dim)\n",
    "    w1 = torch.norm(x1, 2, dim)\n",
    "    w2 = torch.norm(x2, 2, dim)\n",
    "    return (w12 / (w1 * w2).clamp(min=eps)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, context, smooth=1., axis=2):\n",
    "    batch_size_q, queryL = query.shape[0], query.shape[1]\n",
    "    batch_size, sourceL = context.shape[0], context.shape[1]\n",
    "    \n",
    "    queryT = tf.transpose(query, [0, 2, 1])\n",
    "    attn = tf.matmul(context, queryT)\n",
    "    attn = tf.transpose(attn, [0, 2, 1])\n",
    "    attn = tf.reshape(attn, (batch_size*queryL, sourceL))\n",
    "    attn = tf.nn.softmax(attn*smooth)\n",
    "    attn = tf.reshape(attn, (batch_size, queryL, sourceL))\n",
    "    attnT = tf.transpose(attn, [0, 2, 1])\n",
    "    contextT = tf.transpose(context, [0, 2, 1])\n",
    "    weighted_context  = tf.matmul(contextT, attnT)\n",
    "    weighted_context = tf.transpose(weighted_context, [0, 2, 1])\n",
    "    \n",
    "    return weighted_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1.9430578 1.7702808 2.1892307 2.15313   2.212085  1.9590675 2.4017167\n",
      "  1.4632156 1.9711251 2.7198946 1.937081  1.7406503]\n",
      " [2.011984  1.8523271 2.2039373 2.217395  2.300087  1.9902754 2.4442534\n",
      "  1.4377826 2.0331593 2.77314   2.0203288 1.8760084]\n",
      " [1.9343557 1.8249769 2.1832304 2.164532  2.189677  1.9731306 2.3717\n",
      "  1.4712443 1.9308549 2.6850007 1.9665947 1.785289 ]\n",
      " [1.9938582 1.8438002 2.245632  2.2680688 2.2907639 2.0079536 2.4695222\n",
      "  1.4768684 2.0151649 2.7645772 2.0092876 1.875711 ]\n",
      " [2.0941916 1.9954453 2.374743  2.410759  2.3421936 2.109109  2.5892982\n",
      "  1.6595821 2.157721  2.8741732 2.1335208 1.9484286]\n",
      " [2.010512  1.7982444 2.234227  2.2858405 2.2659795 1.9674993 2.4552782\n",
      "  1.4045287 1.9990218 2.7719142 1.9954426 1.8555772]\n",
      " [1.983668  1.842339  2.2284348 2.2312255 2.3161888 2.0030966 2.475127\n",
      "  1.4329965 2.0577526 2.826468  2.023432  1.8789468]\n",
      " [1.9275945 1.7240446 2.1258788 2.1465027 2.1959543 1.8921885 2.3519516\n",
      "  1.3685654 1.8990715 2.6574173 1.9461416 1.7679927]\n",
      " [1.886943  1.7628067 2.1233833 2.1249807 2.2118692 1.8844876 2.3514824\n",
      "  1.3352522 1.9226981 2.6881843 1.9142556 1.7407038]\n",
      " [1.8967373 1.7782073 2.1404727 2.134029  2.201227  1.9067159 2.3887815\n",
      "  1.4104881 1.9176998 2.7056355 1.9065117 1.7565899]\n",
      " [1.9932705 1.7811097 2.1984496 2.2029483 2.2231104 1.9735075 2.455769\n",
      "  1.4499757 1.9931427 2.7401564 1.9934034 1.8083119]\n",
      " [2.0029917 1.81748   2.2154543 2.1897225 2.274968  1.9821804 2.4340909\n",
      "  1.3653307 2.0303578 2.7578564 1.9861314 1.78566  ]], shape=(12, 12), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "videos, sentences, lengths, labels = iter(test_ds).get_next()\n",
    "\n",
    "videos_repr = moment.video_1(videos)\n",
    "sentences_repr = moment.sentence_1(sentences)\n",
    "\n",
    "scores = []\n",
    "\n",
    "for i in range(moment.batch_size):\n",
    "    n_word = lengths[i]\n",
    "    cap_i = sentences_repr[i, :n_word, :]\n",
    "    cap_i_expand = tf.broadcast_to(cap_i, [configs.batch_size, n_word, configs.n_features])\n",
    "    \n",
    "    attended_videos = attention(cap_i_expand, videos_repr, axis=2)\n",
    "    similarity = cosine_similarity(attended_videos, cap_i_expand)\n",
    "#     print(similarity)\n",
    "    score = tf.reduce_logsumexp(similarity, axis=1)\n",
    "    scores.append(score)\n",
    "\n",
    "scores = tf.stack(scores, axis=1)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos, sentences, lengths, labels = iter(test_ds).get_next()\n",
    "\n",
    "videos_repr = moment.video_1(videos)\n",
    "sentences_repr = moment.sentence_1(sentences)\n",
    "\n",
    "scores_videos = []\n",
    "for i in range(moment.batch_size):\n",
    "    _scores_videos = []\n",
    "    \n",
    "    \n",
    "    for j in range(moment.batch_size):\n",
    "        coattention_matrix = moment.matrix_cosine_similarity(videos_repr[i], sentences_repr[j])\n",
    "        scores_video = moment.get_video_score(videos_repr[i], sentences_repr[j], coattention_matrix)\n",
    "\n",
    "        _scores_videos.append(tf.reduce_logsumexp(scores_video))\n",
    "\n",
    "    scores_videos.append(_scores_videos)\n",
    "\n",
    "scores_videos = tf.stack(scores_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(12, 12), dtype=float32, numpy=\n",
       "array([[4.8511205, 4.8986025, 4.8925076, 4.874424 , 4.8867793, 4.900085 ,\n",
       "        4.9110694, 4.9775167, 4.905105 , 4.9022326, 4.9064703, 4.8731217],\n",
       "       [4.944601 , 4.9338574, 4.8743906, 4.8945284, 4.9631333, 4.922152 ,\n",
       "        4.9323125, 4.9232564, 4.943099 , 4.952224 , 4.942667 , 4.945979 ],\n",
       "       [4.853339 , 4.913038 , 4.9157557, 4.860389 , 4.872889 , 4.914023 ,\n",
       "        4.8827047, 4.960747 , 4.8737316, 4.898944 , 4.892063 , 4.8918996],\n",
       "       [4.9080496, 4.9252305, 4.9198675, 4.9207854, 4.955092 , 4.946013 ,\n",
       "        4.927921 , 4.9607573, 4.95114  , 4.9478283, 4.951502 , 4.95385  ],\n",
       "       [5.010769 , 5.0383425, 5.026829 , 5.0751786, 5.0121384, 5.0301247,\n",
       "        5.032574 , 5.095777 , 5.0758386, 5.017351 , 5.0531955, 5.0220013],\n",
       "       [4.936885 , 4.913944 , 4.916531 , 4.9627905, 4.93622  , 4.9168797,\n",
       "        4.941568 , 4.91589  , 4.921717 , 4.9537983, 4.92158  , 4.9398446],\n",
       "       [4.88289  , 4.9135957, 4.874546 , 4.8684583, 4.9542246, 4.9110913,\n",
       "        4.9355083, 4.8859563, 4.9553857, 4.9642   , 4.910773 , 4.9119077],\n",
       "       [4.908105 , 4.872588 , 4.8771043, 4.8835793, 4.918268 , 4.8894906,\n",
       "        4.87997  , 4.909861 , 4.908917 , 4.8930655, 4.9228125, 4.9045715],\n",
       "       [4.888624 , 4.927528 , 4.875794 , 4.888833 , 4.968838 , 4.8879614,\n",
       "        4.9086146, 4.8855453, 4.915458 , 4.925437 , 4.8963275, 4.8849797],\n",
       "       [4.849646 , 4.879707 , 4.845461 , 4.8272934, 4.8882537, 4.8583503,\n",
       "        4.905744 , 4.9118834, 4.869279 , 4.914976 , 4.851061 , 4.84907  ],\n",
       "       [4.9163027, 4.9124246, 4.908942 , 4.900213 , 4.9418416, 4.9589176,\n",
       "        4.931695 , 4.9735184, 4.960732 , 4.950821 , 4.9691215, 4.9455004],\n",
       "       [4.9416637, 4.9354725, 4.9024916, 4.8960533, 4.9885774, 4.9135556,\n",
       "        4.9424624, 4.874145 , 4.9794893, 4.9524894, 4.9238896, 4.896561 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_videos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
